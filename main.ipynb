{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb48838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Documents\\Project\\Contextual_QA_Bot\\Contextual-QA-Bot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Environment Setup\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# New imports for Google Generative AI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS # Still using FAISS for in-memory vector store\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get your Google API key from environment variables\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set your Google API key in a .env file or as an environment variable.\")\n",
    "    # In a notebook, you might want to raise an error to stop execution if the key is critical.\n",
    "    # raise ValueError(\"GOOGLE_API_KEY not found.\")\n",
    "else:\n",
    "    print(\"Google API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1048b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.txt' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Prepare Sample Data\n",
    "\n",
    "data_file_path = \"data.txt\"\n",
    "\n",
    "# Create a dummy data.txt if it doesn't exist for initial testing\n",
    "if not os.path.exists(data_file_path):\n",
    "    with open(data_file_path, \"w\") as f:\n",
    "        f.write(\"\"\"LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). It provides a modular and flexible architecture that allows developers to chain together different components, such as LLM wrappers, prompt templates, document loaders, and vector stores. This enables building complex applications like chatbots, summarization tools, and question-answering systems.\n",
    "\n",
    "One of LangChain's core concepts is Retrieval-Augmented Generation (RAG). RAG systems enhance the capabilities of LLMs by allowing them to retrieve information from an external knowledge base before generating a response. This helps overcome the LLM's inherent limitations, such as outdated knowledge or inability to access specific private data. The process typically involves embedding documents into a vector space, storing them in a vector database, and then retrieving relevant documents based on a user's query. The retrieved documents are then provided as context to the LLM, which uses this context to formulate a more accurate and informed answer.\n",
    "\n",
    "LangChain supports various LLM providers, including OpenAI, Hugging Face, Google, and others. It also integrates with numerous data sources and vector databases. The framework aims to abstract away much of the complexity involved in working directly with LLMs and their ecosystem, making it easier for developers to build powerful AI applications.\n",
    "\"\"\")\n",
    "    print(f\"'{data_file_path}' created with sample content.\")\n",
    "else:\n",
    "    print(f\"'{data_file_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd92090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and chunking text...\n",
      "Original document split into 3 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Text Loading & Chunking\n",
    "\n",
    "print(\"\\nLoading and chunking text...\")\n",
    "loader = TextLoader(data_file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Original document split into {len(chunks)} chunks.\")\n",
    "# Optional: Print the first chunk to inspect\n",
    "# print(\"\\nFirst chunk:\")\n",
    "# print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cc89a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings and creating vector store (FAISS)...\n",
      "Vector store created successfully.\n",
      "--- Phase 1 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Generate Embeddings & Create Vector Store\n",
    "\n",
    "print(\"\\nGenerating embeddings and creating vector store (FAISS)...\")\n",
    "# Initialize Google Generative AI embeddings model\n",
    "# \"models/embedding-001\" is Google's text embedding model.\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key)\n",
    "\n",
    "# Create an in-memory FAISS vector store from the document chunks and embeddings\n",
    "# This step sends your text chunks to Google's embedding API.\n",
    "db = FAISS.from_documents(chunks, embeddings_model)\n",
    "\n",
    "print(\"Vector store created successfully.\")\n",
    "print(\"--- Phase 1 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f90c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Google Generative AI client.\n",
      "\n",
      "--- Available Gemini Models ---\n",
      "Name: models/gemini-1.0-pro-vision-latest, Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Name: models/gemini-pro-vision, Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Name: models/gemini-1.5-pro-latest, Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "Name: models/gemini-1.5-pro-001, Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Name: models/gemini-1.5-pro-002, Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "Name: models/gemini-1.5-pro, Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Name: models/gemini-1.5-flash-latest, Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Name: models/gemini-1.5-flash-001, Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Name: models/gemini-1.5-flash-001-tuning, Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Name: models/gemini-1.5-flash, Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Name: models/gemini-1.5-flash-002, Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "Name: models/gemini-1.5-flash-8b, Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Name: models/gemini-1.5-flash-8b-001, Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Name: models/gemini-1.5-flash-8b-latest, Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Name: models/gemini-1.5-flash-8b-exp-0827, Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Name: models/gemini-1.5-flash-8b-exp-0924, Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Name: models/gemini-2.5-pro-exp-03-25, Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "Name: models/gemini-2.5-pro-preview-03-25, Description: Gemini 2.5 Pro Preview 03-25\n",
      "Name: models/gemini-2.5-flash-preview-04-17, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.5-flash-preview-05-20, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.5-flash-preview-04-17-thinking, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.5-pro-preview-05-06, Description: Preview release (May 6th, 2025) of Gemini 2.5 Pro\n",
      "Name: models/gemini-2.0-flash-exp, Description: Gemini 2.0 Flash Experimental\n",
      "Name: models/gemini-2.0-flash, Description: Gemini 2.0 Flash\n",
      "Name: models/gemini-2.0-flash-001, Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "Name: models/gemini-2.0-flash-exp-image-generation, Description: Gemini 2.0 Flash (Image Generation) Experimental\n",
      "Name: models/gemini-2.0-flash-lite-001, Description: Stable version of Gemini 2.0 Flash Lite\n",
      "Name: models/gemini-2.0-flash-lite, Description: Gemini 2.0 Flash-Lite\n",
      "Name: models/gemini-2.0-flash-preview-image-generation, Description: Gemini 2.0 Flash Preview Image Generation\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05, Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Name: models/gemini-2.0-flash-lite-preview, Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Name: models/gemini-2.0-pro-exp, Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "Name: models/gemini-2.0-pro-exp-02-05, Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "Name: models/gemini-exp-1206, Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n",
      "Name: models/gemini-2.0-flash-thinking-exp-01-21, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.0-flash-thinking-exp, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.0-flash-thinking-exp-1219, Description: Preview release (April 17th, 2025) of Gemini 2.5 Flash\n",
      "Name: models/gemini-2.5-flash-preview-tts, Description: Gemini 2.5 Flash Preview TTS\n",
      "Name: models/gemini-2.5-pro-preview-tts, Description: Gemini 2.5 Pro Preview TTS\n",
      "Name: models/learnlm-2.0-flash-experimental, Description: LearnLM 2.0 Flash Experimental\n",
      "Name: models/gemma-3-1b-it, Description: \n",
      "Name: models/gemma-3-4b-it, Description: \n",
      "Name: models/gemma-3-12b-it, Description: \n",
      "Name: models/gemma-3-27b-it, Description: \n",
      "Name: models/gemma-3n-e4b-it, Description: \n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# New Cell (run this before Cell 5)\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n",
    "    print(\"Please set your Google API key.\")\n",
    "else:\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    print(\"Configured Google Generative AI client.\")\n",
    "\n",
    "    print(\"\\n--- Available Gemini Models ---\")\n",
    "    try:\n",
    "        for m in genai.list_models():\n",
    "            if 'generateContent' in m.supported_generation_methods:\n",
    "                print(f\"Name: {m.name}, Description: {m.description}\")\n",
    "            # Optionally, also check for embedding models if you suspect issues there\n",
    "            # if 'embedContent' in m.supported_generation_methods:\n",
    "            #     print(f\"Name (Embeddings): {m.name}, Description: {m.description}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "        print(\"Please check your GOOGLE_API_KEY and ensure Generative Language API is enabled for your project.\")\n",
    "\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52b7777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Google Gemini LLM...\n",
      "Gemini LLM initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Initialize the LLM (Large Language Model)\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "print(\"\\nInitializing Google Gemini LLM...\")\n",
    "\n",
    "# Initialize the ChatGoogleGenerativeAI model\n",
    "# \"gemini-pro\" is a text-only model suitable for chat and Q&A.\n",
    "# temperature controls randomness: 0 for more deterministic, higher for more creative.\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0, google_api_key=google_api_key)\n",
    "\n",
    "print(\"Gemini LLM initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3c18b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining the RAG prompt template...\n",
      "Prompt template defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define the RAG Prompt Template\n",
    "\n",
    "print(\"\\nDefining the RAG prompt template...\")\n",
    "\n",
    "# This template instructs the LLM to use the provided context to answer the question.\n",
    "# {context} will be filled by the retrieved documents.\n",
    "# {input} will be the user's question.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the user's question based on the below context.\n",
    "If the answer is not in the context, clearly state that you don't have enough information from the provided context.\n",
    "Do not make up information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Prompt template defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "229e90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the document combining chain...\n",
      "Document combining chain created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create the Document Combining Chain\n",
    "\n",
    "print(\"\\nCreating the document combining chain...\")\n",
    "\n",
    "# This chain will take the retrieved documents and the user's question,\n",
    "# combine them according to the prompt template, and send to the LLM.\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "print(\"Document combining chain created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912d3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the retrieval chain...\n",
      "Retrieval chain created.\n",
      "--- Phase 2 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create the Retrieval Chain\n",
    "\n",
    "print(\"\\nCreating the retrieval chain...\")\n",
    "\n",
    "# Get the retriever from our FAISS vector store (db from Phase 1)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Create the full retrieval chain:\n",
    "# 1. User input comes in.\n",
    "# 2. Retriever finds relevant documents from the vector store.\n",
    "# 3. Document chain combines these documents with the prompt and sends to LLM.\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "print(\"Retrieval chain created.\")\n",
    "print(\"--- Phase 2 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Cell (e.g., after Cell 8 in main.ipynb)\n",
    "\n",
    "# --- Core RAG Setup Function ---\n",
    "# This function encapsulates the setup of your RAG chain.\n",
    "# We'll call this from our API file later.\n",
    "\n",
    "def setup_rag_chain():\n",
    "    \"\"\"\n",
    "    Initializes and returns the LangChain RAG retrieval chain\n",
    "    using Google Generative AI embeddings and LLM.\n",
    "    \"\"\"\n",
    "    # Ensure environment variables are loaded if this function is called independently\n",
    "    load_dotenv()\n",
    "    google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "    if not google_api_key:\n",
    "        raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
    "\n",
    "    # --- Re-initialize components (similar to Cells 3, 4, 5, 6, 7, 8) ---\n",
    "    # 1. Text Loading & Chunking\n",
    "    data_file_path = \"data.txt\"\n",
    "    loader = TextLoader(data_file_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 2. Generate Embeddings & Create Vector Store\n",
    "    embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key)\n",
    "    db = FAISS.from_documents(chunks, embeddings_model)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # 3. Initialize LLM\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0, google_api_key=google_api_key)\n",
    "\n",
    "    # 4. Define the RAG Prompt Template\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"Answer the user's question based on the below context.\n",
    "    If the answer is not in the context, clearly state that you don't have enough information from the provided context.\n",
    "    Do not make up information.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {input}\n",
    "    \"\"\")\n",
    "\n",
    "    # 5. Create the Document Combining Chain\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    # 6. Create the Retrieval Chain\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    return retrieval_chain\n",
    "\n",
    "# --- Test the function (Optional, run if you want to test within notebook) ---\n",
    "# print(\"\\nTesting setup_rag_chain function locally...\")\n",
    "# try:\n",
    "#     my_rag_chain = setup_rag_chain()\n",
    "#     test_response = my_rag_chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "#     print(f\"Test Answer: {test_response['answer']}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during local test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad09a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the Q&A system. Ask a question about LangChain or RAG based on the provided text.\n",
      "Type 'exit' to quit.\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n",
      "Answer: I don't have enough information from the provided context to answer who created LangChain.\n",
      "Thinking...\n",
      "Answer: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs).  It provides a modular and flexible architecture allowing developers to chain together different components to build complex applications like chatbots, summarization tools, and question-answering systems.  It supports various LLM providers and integrates with numerous data sources and vector databases.\n",
      "Thinking...\n",
      "Answer: LangChain is a framework designed to simplify the creation of applications using large language models (LLMs).  It provides a modular and flexible architecture allowing developers to chain together different components to build complex applications like chatbots, summarization tools, and question-answering systems.  It supports various LLM providers and integrates with numerous data sources and vector databases.\n",
      "Thinking...\n",
      "Answer: * LangChain's core concept is Retrieval-Augmented Generation (RAG), which enhances LLMs by retrieving information from an external knowledge base before generating a response.\n",
      "\n",
      "* RAG overcomes LLMs' limitations like outdated knowledge and inability to access private data.  This involves embedding documents, storing them in a vector database, and retrieving relevant documents based on user queries.\n",
      "\n",
      "* LangChain is a framework simplifying LLM application creation, offering a modular architecture to chain components like LLM wrappers, prompt templates, document loaders, and vector stores.\n",
      "\n",
      "*  It supports various LLM providers (OpenAI, Hugging Face, Google, etc.) and integrates with many data sources and vector databases.\n",
      "\n",
      "* LangChain aims to simplify working with LLMs, making it easier to build AI applications like chatbots and question-answering systems.\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n",
      "Answer: The provided text does not contain information about \"escape\".\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n",
      "Answer: Please provide the question you would like me to answer based on the provided context.\n",
      "Thinking...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Implement Q&A Functionality\n",
    "\n",
    "print(\"\\nTesting the Q&A system. Ask a question about LangChain or RAG based on the provided text.\")\n",
    "print(\"Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nYour Question: \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Exiting Q&A system. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    print(\"Thinking...\")\n",
    "    # Invoke the retrieval chain with the user's query\n",
    "    # The 'answer' key in the result will contain the LLM's response.\n",
    "    try:\n",
    "        response = retrieval_chain.invoke({\"input\": user_query})\n",
    "        print(f\"Answer: {response['answer']}\")\n",
    "        # Optional: See the retrieved documents that the LLM used\n",
    "        # print(\"\\n--- Retrieved Documents (Context for LLM) ---\")\n",
    "        # for doc in response['context']:\n",
    "        #     print(f\"- {doc.page_content[:200]}...\") # Print first 200 chars of each doc\n",
    "        # print(\"-----------------------------------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please check your API key, internet connection, and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25690a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
